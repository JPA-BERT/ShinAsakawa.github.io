{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "003neural_networks_tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2020ccap/2020-0404neural_networks_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iepfosrqlzwX",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "- source https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
        "- date: 2020-0404\n",
        "- note: for AI_arts\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3ae-G6nknVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAJ_Ws2JknVL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Neural Networks\n",
        "\n",
        "\n",
        "ニューラルネットワークは `` torch.nn``パッケージを使用して構築できます。\n",
        "`autograd` を概説しました。`nn` モジュールは `autograd` に依存して，モデルを定義し，勾配計算を行います。\n",
        "`nn.Module` には層と `output` を返すメソッド `forward(input)` が含まれます。\n",
        "数字画像を分類する LeNet を見てください:\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://pytorch.org/tutorials/_images/mnist.png\"><br/>\n",
        "    2020ccap/mnist.png\n",
        "</div>\n",
        "\n",
        "<!--\n",
        "It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "- Define the neural network that has some learnable parameters (or weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule:\n",
        "  ``weight = weight - learning_rate * gradient``\n",
        "-->\n",
        "\n",
        "これは単純なフィードフォワードネットワークです。 入力を受け取り，いくつかの層を逐次通過し，最終的に出力を提供します。\n",
        "ニューラルネットワークの一般的なトレーニング手順は次のとおりです。\n",
        "\n",
        "- 学習可能なパラメーター（または重み）を持つニューラルネットワークを定義する\n",
        "- 入力のデータセットを反復処理する\n",
        "- ネットワークを通過させて入力情報を処理する\n",
        "- 損失を計算する（出力が正しいことからどれくらい離れているか）\n",
        "- 勾配をネットワークのパラメータに伝播する\n",
        "- 単純な更新則を使用してネットワークの結合係数を更新する: \n",
        "   ``結合係数 = 以前の結合係数 - 学習率 * 勾配``\n",
        "\n",
        "## ネットワークの定義\n",
        "ネットワークを定義してみましょう\n",
        "\n",
        "<!--\n",
        "## Define the network\n",
        "Let’s define this network:\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0inF2I8aknVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8c329b85-6766-4c55-e445-552be12be21e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# 以下で実際にネットワークを定義します\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 入力画像のチャンネル数は 1 すなわち白黒濃淡画像\n",
        "        # 出力チャンネルすなわち特徴数は 6 ,\n",
        "        # 3 x 3 の畳み込みを実施\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)  # 左から順に 入力次元数，出力次元数，カーネルサイズ\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        #  アフィン変換: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 縦横 6*6 の画像が 16 次元の特徴（カーネル）数から 120 次元の出力\n",
        "        self.fc2 = nn.Linear(120, 84)          # 入力次元 120 から 出力 84 次元へ変換\n",
        "        self.fc3 = nn.Linear(84, 10)           # 入力 84 次元から 10 次元へ \n",
        "\n",
        "    def forward(self, x):\n",
        "        # ウィンドウ幅 (2, 2) のマックスプーリング\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # 正方行列でウィンドウを指定するのであれば引数は 1 つでも良い\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # 最初の次元はミニバッチサイズなので除外\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8enW_yJknVO",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "You just have to define the ``forward`` function, and the ``backward``\n",
        "function (where gradients are computed) is automatically defined for you using ``autograd``.\n",
        "You can use any of the Tensor operations in the ``forward`` function.\n",
        "\n",
        "The learnable parameters of a model are returned by ``net.parameters()``\n",
        "-->\n",
        "\n",
        "単に `forward` 関数を定義するだけで `autograd` 機能により，勾配が計算される `backward` 関数は自動的に定義されます。\n",
        "`forward` 関数では テンソル Tensor 操作を使用できます。\n",
        "\n",
        "モデルの学習可能なパラメータは `net.parameters()` によって返されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL42orWwknVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4d536ade-de61-477d-9ff3-f33b2fa2738d"
      },
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQwRWTKknVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWAFravLknVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "cfdefd53-0909-4b7b-ba97-75aa2826c8a3"
      },
      "source": [
        "# keras スタイルのネットワーク要約を出力するパッケージ torchsummary を使います。\n",
        "#!pip install torchsummary\n",
        "import torchsummary\n",
        "torchsummary.summary(net, input_size=(1,32,32))  # input_size の引数は 入力チャンネル数（特徴数）, x (幅), y（高さ）"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 30, 30]              60\n",
            "            Conv2d-2           [-1, 16, 13, 13]             880\n",
            "            Linear-3                  [-1, 120]          69,240\n",
            "            Linear-4                   [-1, 84]          10,164\n",
            "            Linear-5                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 81,194\n",
            "Trainable params: 81,194\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.06\n",
            "Params size (MB): 0.31\n",
            "Estimated Total Size (MB): 0.38\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx8H1YXTknVW",
        "colab_type": "text"
      },
      "source": [
        "<!--Let's try a random 32x32 input.\n",
        "Note: expected input size of this net (LeNet) is 32x32. \n",
        "To use this net on the MNIST dataset, please resize the images from the dataset to 32x32.\n",
        "-->\n",
        "\n",
        "ランダムな 32x32 の入力を試してみましょう。\n",
        "注：このネット（LeNet）の想定入力サイズは 32x32 です。\n",
        "MNIST データセットでこのネットを使用するにはデータセットから 32x32 に画像のサイズを変更してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPk_qH0mknVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "172628eb-23d6-4486-e061-e2f0f234fae6"
      },
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.0691, -0.0019,  0.1468,  0.1439,  0.0606, -0.0514,  0.1081, -0.0990,\n",
            "         -0.1274,  0.0289]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6DRLD2RknVa",
        "colab_type": "text"
      },
      "source": [
        "<!--Zero the gradient buffers of all parameters and backprops with random gradients:-->\n",
        "\n",
        "勾配計算に必要な全パラメータを 0 で初期化し，誤差逆伝播用の勾配を乱数で初期化します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpg_fCF_knVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSK3ClYtknVc",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "<div class=\"alert alert-info\">\n",
        "    <h4>Note</h4><p>\n",
        "    \n",
        "``torch.nn`` only supports mini-batches. The entire ``torch.nn`` package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
        "\n",
        "    For example, ``nn.Conv2d`` will take in a 4D Tensor of ``nSamples x nChannels x Height x Width``.\n",
        "\n",
        "    If you have a single sample, just use ``input.unsqueeze(0)`` to add a fake batch dimension.\n",
        "</p>\n",
        "</div>\n",
        "-->\n",
        "\n",
        "<div class=\"aleart-info\">\n",
        "<h3>注: </h3><p>\n",
        "`torch.nn` はミニバッチのみをサポートします。`torch.nn` パッケージ全体は，単一のサンプルではなく，サンプルのミニバッチである入力のみをサポートします。\n",
        "たとえば `nn.Conv2d` は `nSamples x nChannels x Height x Width` の 4元テンソルを取り込みます。\n",
        "単一のサンプルがある場合は `input.unsqueeze(0)` を使用して偽のミニバッチ次元を追加します。\n",
        "</div>\n",
        "\n",
        "**これまでの復習**:\n",
        "<!--\n",
        "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
        "\n",
        "**Recap:**\n",
        "-->\n",
        "\n",
        "-  ``torch.Tensor`` - 多次元配列。自動微分 ``backward()`` に対応。その勾配を保持している<!--A *multi-dimensional array* with support for autograd operations like ``backward()``. Also *holds the gradient* w.r.t. the tensor.-->\n",
        "-  ``nn.Module`` - ニューラルネットワークモジュール。パラメータの格納，GPU への移動，読み込み，書き出しなどをサポートしている<!--\n",
        "Neural network module. *Convenient way of encapsulating parameters*, with helpers for moving them to GPU, exporting, loading, etc.-->\n",
        "-  ``nn.Parameter`` - PyTorch で用いられるテンソルの一つ。各パラメータの属性や値は PyTorhc で用いられる <!--A kind of Tensor, that is *automatically registered as a parameter when assigned as an attribute to a* ``Module``.-->\n",
        "-  ``autograd.Function`` - ``forward（）`` を定義すれば，自動的に ``backward()`` を計算してくれる。各々の ``Function`` ノードは，``Tensor`` を生成し，その履歴を保持している <--Implements *forward and backward definitions of an autograd operation*. Every ``Tensor`` operation creates at least a single ``Function`` node that connects to functions that created a ``Tensor`` and *encodes its history*.-->\n",
        "\n",
        "\n",
        "**ここまでの説明**:\n",
        "- ニューラルネットワークの定義\n",
        "- 入力と `backword` 演算の処理\n",
        "\n",
        "<!--**At this point, we covered:**\n",
        "  -  Defining a neural network\n",
        "  -  Processing inputs and calling backward\n",
        "-->\n",
        "\n",
        "**この先**:\n",
        "- 損失関数の定義\n",
        "- パラメータの更新(学習)\n",
        "\n",
        "\n",
        "<!--\n",
        "**Still Left:**\n",
        "  -  Computing the loss\n",
        "  -  Updating the weights of the network\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpzWJdNuknVd",
        "colab_type": "text"
      },
      "source": [
        "## 損失関数\n",
        "<!--\n",
        "## Loss Function\n",
        "-->\n",
        "\n",
        "- <font color=\"teal\">損失関数</font>は，出力とターゲット（正解）の対を入力して，出力がターゲットとどのくらい離れているかを計算します。\n",
        "- PyTorch では損失関数として，あらかじめ，数種類定義されています。 詳細は <https://pytorch.org/docs/nn.html#loss-functions> を御覧ください。\n",
        "もっとも簡単な平均自乗誤差関数は ``nn.MSELoss`` です。\n",
        "\n",
        "<!--\n",
        "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different `loss functions` <https://pytorch.org/docs/nn.html#loss-functions> under the\n",
        "nn package.\n",
        "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error between the input and the target.\n",
        "\n",
        "For example:\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26cf8R9HknVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#help(torch.nn)\n",
        "#help(torch.nn.functional)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vvr1xssknVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58a49fe4-84b2-4c78-bc3e-2e3543477488"
      },
      "source": [
        "output = net(input)\n",
        "target = torch.randn(10)  # ダミーの値を定義\n",
        "target = target.view(1, -1)  # 出力と同じ次元にする\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5948, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf7TWw6cknVh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b690d841-ecb8-4c9b-8ed5-f86220748c3f"
      },
      "source": [
        "# 少し分かりづらいかも知れないので解説，基本単位が必ずミニバッチサイズなので，Tensor の次元は 1 次元ではありえない\n",
        "# ですので，次元を揃えるために target.view(1,-1)  をします。\n",
        "output.size()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-KD4vbnknVk",
        "colab_type": "text"
      },
      "source": [
        "損失関数 ``loss`` を逆伝播させるために ``.grad_fn`` を使います。計算グラフとしては以下のようになります。\n",
        "<!--\n",
        "Now, if you follow ``loss`` in the backward direction, using its ``.grad_fn`` attribute, you will see a graph of computations that lookslike this:\n",
        "-->\n",
        "\n",
        "```\n",
        "\n",
        "    input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "          -> view -> linear -> relu -> linear -> relu -> linear\n",
        "          -> MSELoss\n",
        "          -> loss\n",
        "```\n",
        "\n",
        "``loss.backword()`` を呼び出すと，あらかじめ定義してあった ``損失`` 値に基づいて，計算グラフ全体が微分されます。\n",
        "``requires_grad=True`` と設定されている，計算グラフ中の全テンソルが計算されます。``.grad`` テンソルには累積勾配が保持されます。\n",
        "\n",
        "<!--\n",
        "So, when we call ``loss.backward()``, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True`` will have their ``.grad`` Tensor accumulated with the gradient.\n",
        "-->\n",
        "\n",
        "実習:\n",
        "<!--\n",
        "For illustration, let us follow a few steps backward:\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArYpzDn5knVl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "024a91e1-275a-4408-bce9-85eb4014cc73"
      },
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MseLossBackward object at 0x7f025ee130f0>\n",
            "<AddmmBackward object at 0x7f025ee130f0>\n",
            "<AccumulateGrad object at 0x7f025edf85c0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWLL8ntJknVn",
        "colab_type": "text"
      },
      "source": [
        "# 誤差逆伝播\n",
        "<!--\n",
        "# Backprop\n",
        "\n",
        "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
        "You need to clear the existing gradients though, else gradients will be\n",
        "accumulated to existing gradients.\n",
        "-->\n",
        "\n",
        "\n",
        "``loss.backward()`` を呼び出して，`conv1` のバイアスを確認しまししょう。\n",
        "<!--\n",
        "Now we shall call ``loss.backward()``, and have a look at conv1's bias gradients before and after the backward.\n",
        "-->\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLGMSc53knVn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b1f78eb6-a7e8-487f-99b0-23dfce964a87"
      },
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0085, -0.0063,  0.0046,  0.0052, -0.0058, -0.0040])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebis6rOmknVp",
        "colab_type": "text"
      },
      "source": [
        "## 損失関数の用い方\n",
        "\n",
        "ほとんどのニューラルネットワークフレームワークでは あらかじめ頻用される損失関数 loss functions が定義されています。\n",
        "詳細は [https://pytorch.org/docs/nn](https://pytorch.org/docs/nn) を御覧ください。\n",
        "\n",
        "**The only thing left to learn is:**\n",
        "\n",
        "## 結合係数の更新 （学習，あるいは 訓練）\n",
        "\n",
        "確率的勾配降下法 (SGD) の簡単な場合は以下のようになります:\n",
        "\n",
        "     ``weight = weight - learning_rate * gradient``\n",
        "\n",
        "Python のコードでは以下のようになります\n",
        "\n",
        "```python\n",
        "    learning_rate = 0.01\n",
        "    for f in net.parameters():\n",
        "        f.data.sub_(f.grad.data * learning_rate)\n",
        "```\n",
        "\n",
        "それ以外の更新方法，しばしば最適化手法と呼ばれます。\n",
        "PyTorch では ``torch.optim`` で定義されています。\n",
        "<!--\n",
        "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. \n",
        "To enable this, we built a small package: ``torch.optim`` that implements all these methods. \n",
        "Using it is very simple:\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQt1gJJFknVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIdE0sKcknVs",
        "colab_type": "text"
      },
      "source": [
        "<!--\n",
        "Observe how gradient buffers had to be manually set to zero using  ``optimizer.zero_grad()``. \n",
        "This is because gradients are accumulated  as explained in the `Backprop`section.\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0zJmo14knVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}