{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-0521ccap_softmax_with_temparature.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShinAsakawa/ShinAsakawa.github.io/blob/master/2020ccap/2020_0521ccap_softmax_with_temparature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyPjjkikRV7V",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# ccap 勉強会資料\n",
        "- date: 2020-0521\n",
        "- author: 浅川伸一\n",
        "- title: 注意に使用されるソフトマックス関数は，温度を付けることができる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTEpWqPQkQSh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "- 鈴木さんに解説していただいたとおり，注意とはソフトマックス関数のことです。\n",
        "- そして，ソフトマックス関数とは，複数の選択肢の中から，最大の値を与える選択肢の値を強調し，\n",
        "- かつ，全体の総和が 1 となるような変換を指します。\n",
        "- 従って，どのような値であれ，実数値 スカラ であれば，複数の選択肢の選択確率を与えます。\n",
        "- また，よく知られているとおり，選択する確率を $p$ とすれば，選択しない確率は $1-p$ ですから，\n",
        "- この 2 つの値でソフトマックス関数を行った場合をシグモイド関数 (the logistic sigmoid function) と呼びます。\n",
        "$$\n",
        "\\frac{e^p}{e^p + e^{1-p}} = \\frac{1}{1+e^{\\frac{1-p}{p}}}\n",
        "= \\frac{1}{1+e^{-\\frac{p}{1-p}}}\n",
        "$$\n",
        "上式で，選択確率 $p$ と非選択確率 $(1-p)$ の比 $\\displaystyle\\frac{p}{1-p}$ を $x$ とおくと\n",
        "よく知られたシグモイド関数の定義式を得ます。\n",
        "\n",
        "$$\n",
        "f(x) =\\sigma(x) = \\left[1+e^{-x}\\right]^{-1}\n",
        "$$\n",
        "\n",
        "すなわち，ニューラルネットワークで以前頻繁に用いられていたシグモイド関数は，ソフトマックス関数の特殊な場合であると見なすことが可能です。\n",
        "\n",
        "- 加えて，LSTM や GRU に出てくる **ゲート** とは，シグモイド関数ですから，この値を信用するか，あるいは信用しないかを，表す比を使って，情報を流すか，流さないかを制御するカラクリであると考えることができます。\n",
        "\n",
        "- たとえば，絵画呼称課題において，脳内に想起された回答候補を選択する確率が，その内部刺激表現の強度のみならず，\n",
        "この温度にも影響を受けると考えれば，故障障害の記述に洞察を与えることができるだろうと考えます。\n",
        "- そして，この温度パラメータに相当する，神経対応物はセロトニンやサイトカインなどの神経修飾物質の脳内濃度ではないかと考えることができます。（浅川の妄想です）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4msnwtM1RS5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from scipy.misc import # 古い python のバージョンでは logsumexp の定義されている場所が異なります。\n",
        "from scipy.special import logsumexp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1KvOsdWS1Tq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_wT(x, T=1., axis=None):\n",
        "    \"\"\"\n",
        "    Softmax function\n",
        "\n",
        "    The softmax function transforms each element of a collection by\n",
        "    computing the exponential of each element divided by the sum of the\n",
        "    exponentials of all the elements.  That is, if `x` is a one-dimensional\n",
        "    numpy array::\n",
        "\n",
        "        softmax(x) = np.exp(x)/sum(np.exp(x))\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : array_like\n",
        "        Input array.\n",
        "    axis : int or tuple of ints, optional\n",
        "        Axis to compute values along. Default is None and softmax will be\n",
        "        computed over the entire array `x`.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    s : ndarray\n",
        "        An array the same shape as `x`. The result will sum to 1 along the\n",
        "        specified axis.\n",
        "    \"\"\"\n",
        "\n",
        "    return np.exp(x/T - logsumexp(x/T, axis=axis, keepdims=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJK-9Q8sRvkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(3)  # 乱数\b生成器の種を設定\n",
        "\n",
        "N = 3  # 生成する乱数の個数\n",
        "raw_data = np.random.rand(N)  \n",
        "print(raw_data, softmax_wT(raw_data), softmax_wT(raw_data, T=10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvO0_I51TWED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "width = 0.3\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.bar(x - width, softmax_wT(raw_data, T=0.1), width, label='softmax T=0.1')\n",
        "plt.bar(x,         softmax_wT(raw_data, T=1), width, label='softmax T=1')\n",
        "plt.bar(x + width, softmax_wT(raw_data, T=30), width, label='softmax T=30')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlUvV8-1Tl8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}